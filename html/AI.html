<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Projects - Binghao WANG</title>
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:wght@400;600;700&family=Outfit:wght@300;400;500;600&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="../css/pages.css">
</head>
<body>
    <header class="page-header">
        <div class="page-header-inner">
            <h1><a href="../index.html">Binghao WANG</a></h1>
            <a href="../index.html" class="back-link">‚Üê Back to Home</a>
        </div>
    </header>

    <main class="page-content">
        <h1 class="page-title">AI Projects</h1>
        
        <div class="project-list">

            <article class="project-card">
                <div class="project-card-content">
                    <h2 class="project-card-title">Practical Local Deployment of a "Stanford AI Town" Model: Multi-Agent Memory, Reasoning, and Social Simulation</h2>
                    <p class="project-card-description">
                        "Stanford AI Town" is a Generative Agents project that explores how multiple AI agents with memory, reasoning, and autonomous decision-making capabilities can operate long-term in a virtual environment with complete infrastructure, and "live" and "socialize" in a human-like manner.Based on the Microverse project, I implemented similar functionality on my local server and observed the development of autonomous relationships among NPCs.
                    </p>
                </div>
                <img src="../image/Stanford AI Town.png" alt="Sign Language Recognition">
            </article>

            <article class="project-card">
                <div class="project-card-content">
                    <h2 class="project-card-title">IK-Augmented Sign Language: Scalable Training for Deep Recognition Models</h2>
                    <p class="project-card-description">
                        I present a kinematics-driven data augmentation framework for sign language recognition that employs inverse kinematics (IK) within a 3D simulation environment to generate anatomically coherent gesture sequences. By reconstructing full-body poses from MediaPipe keypoints and introducing multimodal perturbations (e.g., spatial jitter, depth offset, IK sensitivity), our method enriches gesture variability beyond traditional augmentation. Extensive experiments across five deep learning models demonstrate consistent performance gains, particularly with Transformer-based architectures.This article was published in the journal "Algorithms".
                    </p>
                </div>
                <img src="../image/AI5.png" alt="Sign Language Recognition">
            </article>

            <article class="project-card">
                <div class="project-card-content">
                    <h2 class="project-card-title">Reproduce the project to interact with the virtual environment</h2>
                    <p class="project-card-description">
                        AlfWorld is a reinforcement learning environment for training AI to complete tasks in 3D scenes (implemented by Unity) or text environments (TextWorld). I used this project to implement its text interaction and 3D environment interaction, so that AI can be used to control virtual characters to complete some tasks.
                    </p>
                </div>
                <img src="../image/AI4.png" alt="AlfWorld Project">
            </article>

            <article class="project-card">
                <div class="project-card-content">
                    <h2 class="project-card-title">Fine-tuning the BERT model for sentiment analysis</h2>
                    <p class="project-card-description">
                        BERT is a deep learning model, specifically a pre-trained natural language processing (NLP) model based on the Transformer architecture. By fine-tuning the classifier layer of BERT on the Chinese and English datasets of sentiment classification to adapt to the task of sentiment analysis, a terminal interactive script is written to call the fine-tuned trained BERT model, so that the emotional state of the sentence can be returned when a sentence is input in the terminal. The details will be explained in the blog.
                    </p>
                </div>
                <img src="../image/AI2.png" alt="BERT Sentiment Analysis">
            </article>

            <article class="project-card">
                <div class="project-card-content">
                    <h2 class="project-card-title">Image Processing-based Workpiece Shape Defect Detection Technology</h2>
                    <p class="project-card-description">
                        In this experiment, the optical disc was processed through a series of processes such as grayscale conversion and image enhancement, and then the improved Canny algorithm was used for feature recognition, and then the circularity of the defective part was calculated to determine the defect. A GUI was designed to import images and perform the above operations, and finally 50 workpiece images were used for detection to obtain the accuracy.
                    </p>
                </div>
                <img src="../image/AI1.png" alt="Defect Detection">
            </article>

            <article class="project-card">
                <div class="project-card-content">
                    <h2 class="project-card-title">MATLAB App Designer-based Electronic Yangqin Simulator</h2>
                    <p class="project-card-description">
                        This experiment mainly involves sound signal processing. The original yangqin sound is analyzed through FFT transformation, and three methods, namely FIR low-pass filtering, median filtering and wavelet noise reduction, are compared. Finally, FIR low-pass filtering is selected as the best noise reduction solution. The second is application development. An interactive interface with three modules, namely teaching demonstration, electronic yangqin performance and music score selection, is designed. Through callback functions, functions such as animation teaching, performance simulation and music score playback are realized, which digitizes the learning and performance of the traditional yangqin instrument.
                    </p>
                </div>
                <img src="../image/AI3.png" alt="Electronic Yangqin">
            </article>
        </div>
    </main>
</body>
</html>
